'\" t
.\"     Title: table_dispatcher
.\"    Author: [FIXME: author] [see http://docbook.sf.net/el/author]
.\" Generator: DocBook XSL Stylesheets v1.75.2 <http://docbook.sf.net/>
.\"      Date: 03/13/2012
.\"    Manual: \ \&
.\"    Source: \ \&
.\"  Language: English
.\"
.TH "TABLE_DISPATCHER" "1" "03/13/2012" "\ \&" "\ \&"
.\" -----------------------------------------------------------------
.\" * Define some portability stuff
.\" -----------------------------------------------------------------
.\" ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.\" http://bugs.debian.org/507673
.\" http://lists.gnu.org/archive/html/groff/2009-02/msg00013.html
.\" ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\" -----------------------------------------------------------------
.\" * set default formatting
.\" -----------------------------------------------------------------
.\" disable hyphenation
.nh
.\" disable justification (adjust text to left margin only)
.ad l
.\" -----------------------------------------------------------------
.\" * MAIN CONTENT STARTS HERE *
.\" -----------------------------------------------------------------
.SH "NAME"
table_dispatcher \- PgQ consumer that is used to write source records into partitoned table\&.
.SH "SYNOPSIS"
.sp
.nf
table_dispatcher\&.py [switches] config\&.ini
.fi
.SH "DESCRIPTION"
.sp
table_dispatcher is PgQ consumer that reads url encoded records from source queue and writes them into partitioned tables according to configuration file\&. Used to partiton data\&. For example change log\(cqs that need to kept online only shortly can be written to daily tables and then dropped as they become irrelevant\&. Also allows to select which columns have to be written into target database Creates target tables according to configuration file as needed\&.
.SH "QUICK-START"
.sp
Basic table_dispatcher setup and usage can be summarized by the following steps:
.sp
.RS 4
.ie n \{\
\h'-04' 1.\h'+01'\c
.\}
.el \{\
.sp -1
.IP "  1." 4.2
.\}
PgQ must be installed in source database\&. See pgqadm man page for details\&. Target database must have
pgq_ext
schema installed\&.
.RE
.sp
.RS 4
.ie n \{\
\h'-04' 2.\h'+01'\c
.\}
.el \{\
.sp -1
.IP "  2." 4.2
.\}
edit a table_dispatcher configuration file, say table_dispatcher_sample\&.ini
.RE
.sp
.RS 4
.ie n \{\
\h'-04' 3.\h'+01'\c
.\}
.el \{\
.sp -1
.IP "  3." 4.2
.\}
create source queue
.sp
.if n \{\
.RS 4
.\}
.nf
$ pgqadm\&.py ticker\&.ini create <queue>
.fi
.if n \{\
.RE
.\}
.RE
.sp
.RS 4
.ie n \{\
\h'-04' 4.\h'+01'\c
.\}
.el \{\
.sp -1
.IP "  4." 4.2
.\}
launch table dispatcher in daemon mode
.sp
.if n \{\
.RS 4
.\}
.nf
$ table_dispatcher\&.py table_dispatcher_sample\&.ini \-d
.fi
.if n \{\
.RE
.\}
.RE
.sp
.RS 4
.ie n \{\
\h'-04' 5.\h'+01'\c
.\}
.el \{\
.sp -1
.IP "  5." 4.2
.\}
start producing events
.RE
.SH "CONFIG"
.SS "Common configuration parameters"
.PP
job_name
.RS 4
Name for particulat job the script does\&. Script will log under this name to logdb/logserver\&. The name is also used as default for PgQ consumer name\&. It should be unique\&.
.RE
.PP
pidfile
.RS 4
Location for pid file\&. If not given, script is disallowed to daemonize\&.
.RE
.PP
logfile
.RS 4
Location for log file\&.
.RE
.PP
loop_delay
.RS 4
If continuisly running process, how long to sleep after each work loop, in seconds\&. Default: 1\&.
.RE
.PP
connection_lifetime
.RS 4
Close and reconnect older database connections\&.
.RE
.PP
log_count
.RS 4
Number of log files to keep\&. Default: 3
.RE
.PP
log_size
.RS 4
Max size for one log file\&. File is rotated if max size is reached\&. Default: 10485760 (10M)
.RE
.PP
use_skylog
.RS 4
If set, search for
[\&./skylog\&.ini, ~/\&.skylog\&.ini, /etc/skylog\&.ini]\&. If found then the file is used as config file for Pythons
logging
module\&. It allows setting up fully customizable logging setup\&.
.RE
.SS "Common PgQ consumer parameters"
.PP
pgq_queue_name
.RS 4
Queue name to attach to\&. No default\&.
.RE
.PP
pgq_consumer_id
.RS 4
Consumers ID to use when registering\&. Default: %(job_name)s
.RE
.SS "table_dispatcher parameters"
.PP
src_db
.RS 4
Source database\&.
.RE
.PP
dst_db
.RS 4
Target database\&.
.RE
.PP
dest_table
.RS 4
Where to put data\&. when partitioning, will be used as base name
.RE
.PP
part_field
.RS 4
date field with will be used for partitioning\&.
.RE
.PP
part_template
.RS 4
SQL code used to create partition tables\&. Various magic replacements are done there:
.RE
.PP
_PKEY
.RS 4
comma separated list of primery key columns\&.
.RE
.PP
_PARENT
.RS 4
schema\-qualified parent table name\&.
.RE
.PP
_DEST_TABLE
.RS 4
schema\-qualified partition table\&.
.RE
.PP
_SCHEMA_TABLE
.RS 4
same as
\fIDEST_TABLE but dots replaced with "_\fR", to allow use as index names\&.
.RE
.SS "Example config"
.sp
.if n \{\
.RS 4
.\}
.nf
[table_dispatcher]
job_name          = table_dispatcher_source_table_targetdb
.fi
.if n \{\
.RE
.\}
.sp
.if n \{\
.RS 4
.\}
.nf
src_db            = dbname=sourcedb
dst_db            = dbname=targetdb
.fi
.if n \{\
.RE
.\}
.sp
.if n \{\
.RS 4
.\}
.nf
pgq_queue_name    = sourceq
.fi
.if n \{\
.RE
.\}
.sp
.if n \{\
.RS 4
.\}
.nf
logfile           = log/%(job_name)s\&.log
pidfile           = pid/%(job_name)s\&.pid
.fi
.if n \{\
.RE
.\}
.sp
.if n \{\
.RS 4
.\}
.nf
# where to put data\&.  when partitioning, will be used as base name
dest_table        = orders
.fi
.if n \{\
.RE
.\}
.sp
.if n \{\
.RS 4
.\}
.nf
# names of the fields that must be read from source records
fields            = id, order_date, customer_name
.fi
.if n \{\
.RE
.\}
.sp
.if n \{\
.RS 4
.\}
.nf
# date field with will be used for partitioning
part_field        = order_date
.fi
.if n \{\
.RE
.\}
.sp
.if n \{\
.RS 4
.\}
.nf
# template used for creating partition tables
part_template     =
     create table _DEST_TABLE () inherits (orders);
     alter table only _DEST_TABLE add constraint _DEST_TABLE_pkey primary key (id);
     grant select on _DEST_TABLE to group reporting;
.fi
.if n \{\
.RE
.\}
.SH "COMMAND LINE SWITCHES"
.sp
Following switches are common to all skytools\&.DBScript\-based Python programs\&.
.PP
\-h, \-\-help
.RS 4
show help message and exit
.RE
.PP
\-q, \-\-quiet
.RS 4
make program silent
.RE
.PP
\-v, \-\-verbose
.RS 4
make program more verbose
.RE
.PP
\-d, \-\-daemon
.RS 4
make program go background
.RE
.sp
Following switches are used to control already running process\&. The pidfile is read from config then signal is sent to process id specified there\&.
.PP
\-r, \-\-reload
.RS 4
reload config (send SIGHUP)
.RE
.PP
\-s, \-\-stop
.RS 4
stop program safely (send SIGINT)
.RE
.PP
\-k, \-\-kill
.RS 4
kill program immidiately (send SIGTERM)
.RE
.SH "LOGUTRIGA EVENT FORMAT"
.sp
PgQ trigger function pgq\&.logutriga() sends table change event into queue in following format:
.PP
ev_type
.RS 4

(op || ":" || pkey_fields)\&. Where op is either "I", "U" or "D", corresponging to insert, update or delete\&. And
pkey_fields
is comma\-separated list of primary key fields for table\&. Operation type is always present but pkey_fields list can be empty, if table has no primary keys\&. Example:
I:col1,col2
.RE
.PP
ev_data
.RS 4
Urlencoded record of data\&. It uses db\-specific urlecoding where existence of
\fI=\fR
is meaningful \- missing
\fI=\fR
means NULL, present
\fI=\fR
means literal value\&. Example:
id=3&name=str&nullvalue&emptyvalue=
.RE
.PP
ev_extra1
.RS 4
Fully qualified table name\&.
.RE
